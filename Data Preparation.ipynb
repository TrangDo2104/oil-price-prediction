{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import NoSuchElementException, TimeoutException\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "import time\n",
    "import pandas as pd\n",
    "import re\n",
    "from datetime import datetime, timedelta\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import yfinance as yf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the ticker symbol for Brent Crude Oil\n",
    "ticker = \"BZ=F\"\n",
    "\n",
    "# Get the data for the ticker\n",
    "data = yf.Ticker(ticker)\n",
    "\n",
    "# Get the entire history data by specifying 'max' period\n",
    "history = data.history(period=\"max\")\n",
    "\n",
    "# Check if 'Adj Close' is available, if not, use 'Close' as 'Adj Close'\n",
    "if 'Adj Close' not in history.columns:\n",
    "    history['Adj Close'] = history['Close']\n",
    "\n",
    "# Select the required columns: 'Open', 'High', 'Low', 'Close', 'Adj Close', and 'Volume'\n",
    "filtered_history = history[['Open', 'High', 'Low', 'Close', 'Adj Close', 'Volume']]\n",
    "\n",
    "# Display the filtered data\n",
    "print(filtered_history)\n",
    "\n",
    "# Optional: Save the data to a CSV file\n",
    "filtered_history.to_csv(\"NFLX.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Chrome driver\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "# Open the PropertyGuru rental listings page\n",
    "driver.get(\"https://oilprice.com/Energy/Crude-Oil/\")\n",
    "time.sleep(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_max_page_number(driver):\n",
    "    try:\n",
    "        # Find the \"Last\" button\n",
    "        last_button = driver.find_element(By.CSS_SELECTOR, 'a.last')\n",
    "        \n",
    "        # Extract the href attribute\n",
    "        last_page_url = last_button.get_attribute('href')\n",
    "        \n",
    "        # Use regular expression to extract the page number from the URL\n",
    "        match = re.search(r'Page-(\\d+)\\.html$', last_page_url)\n",
    "        if match:\n",
    "            max_page_number = int(match.group(1))\n",
    "            return max_page_number\n",
    "        else:\n",
    "            print(\"Could not find page number in the URL.\")\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        print(\"Error finding the last page button:\", e)\n",
    "        return None\n",
    "    \n",
    "def scrape_oil_news(driver):\n",
    "    result = []\n",
    "    max_page_number = get_max_page_number(driver)\n",
    "    wait = WebDriverWait(driver, 10)  # WebDriverWait instance with a 10-second timeout\n",
    "\n",
    "    for current_page in tqdm(range(1, max_page_number + 1), desc=\"Scraping Pages\"):\n",
    "        print(f\"Scraping page {current_page}/{max_page_number}\")\n",
    "\n",
    "        # Scrape articles on the current page\n",
    "        try:\n",
    "            # Wait for articles to be present on the page\n",
    "            articles = wait.until(EC.presence_of_all_elements_located((By.CLASS_NAME, 'categoryArticle__content')))\n",
    "        except Exception as e:\n",
    "            print(\"Error locating articles:\", e)\n",
    "            break\n",
    "\n",
    "        # Loop through each article and extract the required information\n",
    "        for article in articles:\n",
    "            try:\n",
    "                link_element = article.find_element(By.CSS_SELECTOR, 'a')\n",
    "                article_url = link_element.get_attribute('href')\n",
    "                article_title = article.find_element(By.CSS_SELECTOR, 'h2.categoryArticle__title').text\n",
    "\n",
    "                # Extract date\n",
    "                date_element = article.find_element(By.CSS_SELECTOR, 'p.categoryArticle__meta')\n",
    "                article_date = date_element.text.split('|')[0].strip()  # Extract only the date part\n",
    "\n",
    "                result.append({\n",
    "                        \"url\": article_url,\n",
    "                        \"title\": article_title,\n",
    "                        \"date\": article_date\n",
    "                    })\n",
    "            except Exception as e:\n",
    "                print(\"Error extracting article data:\", e)\n",
    "        print(f\"Finish page {current_page}\")\n",
    "        # Navigate to the next page\n",
    "        if current_page < max_page_number:\n",
    "            next_button = wait.until(EC.element_to_be_clickable((By.CLASS_NAME, 'next')))\n",
    "            driver.execute_script(\"arguments[0].scrollIntoView(true);\", next_button)\n",
    "            next_button.click()\n",
    "            time.sleep(2)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final = scrape_oil_news(driver)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/changchoichang/Library/Mobile Documents/com~apple~CloudDocs/Documents/GitHub/oil-price-prediction/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Analyzing sentiment: 100%|██████████| 6600/6600 [01:55<00:00, 56.93 rows/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New CSV file 'aggregated_oil_energy_sentiment.csv' has been created. Sentiment analysis completed successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "/var/folders/0p/lnktjx254_z2nwf92thzm_hr0000gn/T/ipykernel_28913/3758875253.py:75: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df['sentiment_score'].fillna(0, inplace=True)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "\n",
    "# Function to load the tokenizer and model\n",
    "def load_model():\n",
    "    try:\n",
    "        # Load tokenizer and model\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
    "        return tokenizer, model\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading model: {e}\")\n",
    "        return None, None\n",
    "\n",
    "# Function to perform sentiment analysis on a text input\n",
    "def analyze_sentiment(text, tokenizer, model):\n",
    "    try:\n",
    "        # Tokenize the input text\n",
    "        inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "        \n",
    "        # Perform the prediction\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "        \n",
    "        # Get the logits (model's raw output)\n",
    "        logits = outputs.logits\n",
    "        \n",
    "        # Convert logits to sentiment prediction\n",
    "        sentiment = torch.argmax(logits, dim=1).item()\n",
    "        \n",
    "        # Define the sentiment categories\n",
    "        sentiment_map = {0: -1, 1: 1}  # Adjusting to return -1 for negative, +1 for positive\n",
    "        \n",
    "        # Return the corresponding sentiment score\n",
    "        return sentiment_map[sentiment]\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error during sentiment analysis: {e}\")\n",
    "        return None\n",
    "\n",
    "# Function to process the CSV and apply sentiment analysis\n",
    "def process_csv(file_path):\n",
    "    # Load the tokenizer and model\n",
    "    tokenizer, model = load_model()\n",
    "    \n",
    "    # Check if the model was loaded successfully\n",
    "    if tokenizer is None or model is None:\n",
    "        print(\"Model loading failed. Exiting.\")\n",
    "        return\n",
    "    \n",
    "    # Load the CSV file\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Convert the 'date' column to datetime objects (ignoring time for aggregation purposes)\n",
    "    df['date'] = pd.to_datetime(df['date'], format=\"%b %d, %Y at %H:%M\")\n",
    "    df['date'] = df['date'].dt.date  # Keep only the date part for grouping\n",
    "    \n",
    "    # Initialize the tqdm progress bar\n",
    "    tqdm_bar = tqdm(total=len(df), desc=\"Analyzing sentiment\", unit=\" rows\")\n",
    "    \n",
    "    # Apply sentiment analysis to each title with a progress bar\n",
    "    df['sentiment_score'] = df['title'].apply(lambda x: analyze_sentiment(x, tokenizer, model))\n",
    "    \n",
    "    # Update the progress bar after each row is processed\n",
    "    for _ in df.index:\n",
    "        tqdm_bar.update(1)\n",
    "    \n",
    "    # Close the progress bar after completion\n",
    "    tqdm_bar.close()\n",
    "    \n",
    "    # If any sentiment analysis failed, fill with 0 (neutral)\n",
    "    df['sentiment_score'].fillna(0, inplace=True)\n",
    "    \n",
    "    # Aggregating data by date\n",
    "    aggregated_df = df.groupby('date').agg(\n",
    "        P_average=('sentiment_score', 'mean'),\n",
    "        P_sum=('sentiment_score', 'sum'),\n",
    "        news_count=('title', 'count')  # Counting the number of news articles\n",
    "    ).reset_index()\n",
    "    \n",
    "    # Sort the result by date (from farthest to nearest)\n",
    "    aggregated_df = aggregated_df.sort_values(by='date', ascending=False)\n",
    "    \n",
    "    # Save the aggregated data into a new CSV file\n",
    "    output_file = 'aggregated_oil_energy_sentiment.csv'\n",
    "    aggregated_df.to_csv(output_file, index=False)\n",
    "    \n",
    "    # Print a message when done\n",
    "    print(f\"New CSV file '{output_file}' has been created. Sentiment analysis completed successfully.\")\n",
    "\n",
    "# Main function to run the process\n",
    "if __name__ == \"__main__\":\n",
    "    # Path to the input CSV file\n",
    "    file_path = 'oil_energy_data.csv'\n",
    "    \n",
    "    # Process the CSV and perform sentiment analysis\n",
    "    process_csv(file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collected 6 tweets so far.\n",
      "Collected 12 tweets so far.\n",
      "Collected 18 tweets so far.\n",
      "Collected 24 tweets so far.\n",
      "Collected 30 tweets so far.\n",
      "Collected 36 tweets so far.\n",
      "Collected 42 tweets so far.\n",
      "Collected 48 tweets so far.\n",
      "Collected 54 tweets so far.\n",
      "Collected 60 tweets so far.\n",
      "Collected 66 tweets so far.\n",
      "Collected 72 tweets so far.\n",
      "Collected 78 tweets so far.\n",
      "Collected 84 tweets so far.\n",
      "Collected 90 tweets so far.\n",
      "Collected 96 tweets so far.\n",
      "Collected 102 tweets so far.\n",
      "Collected 108 tweets so far.\n",
      "Collected 114 tweets so far.\n",
      "Collected 120 tweets so far.\n",
      "Collected 126 tweets so far.\n",
      "Collected 132 tweets so far.\n",
      "Collected 138 tweets so far.\n",
      "Collected 144 tweets so far.\n",
      "Collected 150 tweets so far.\n",
      "Collected 156 tweets so far.\n",
      "Collected 162 tweets so far.\n",
      "Collected 168 tweets so far.\n",
      "Collected 174 tweets so far.\n",
      "Collected 180 tweets so far.\n",
      "Collected 186 tweets so far.\n",
      "Collected 192 tweets so far.\n",
      "Collected 198 tweets so far.\n",
      "Collected 204 tweets so far.\n",
      "Collected 210 tweets so far.\n",
      "Collected 216 tweets so far.\n",
      "Collected 222 tweets so far.\n",
      "Collected 228 tweets so far.\n",
      "Collected 234 tweets so far.\n",
      "Collected 240 tweets so far.\n",
      "Collected 246 tweets so far.\n",
      "Collected 252 tweets so far.\n",
      "Collected 258 tweets so far.\n",
      "Collected 264 tweets so far.\n",
      "Collected 270 tweets so far.\n",
      "Collected 276 tweets so far.\n",
      "Collected 282 tweets so far.\n",
      "Collected 288 tweets so far.\n",
      "Collected 294 tweets so far.\n",
      "Collected 300 tweets so far.\n",
      "Collected 306 tweets so far.\n",
      "Collected 312 tweets so far.\n",
      "Collected 318 tweets so far.\n",
      "Collected 324 tweets so far.\n",
      "Collected 330 tweets so far.\n",
      "Collected 336 tweets so far.\n",
      "Collected 342 tweets so far.\n",
      "Collected 348 tweets so far.\n",
      "Collected 354 tweets so far.\n",
      "Collected 360 tweets so far.\n",
      "Collected 366 tweets so far.\n",
      "Collected 372 tweets so far.\n",
      "Collected 378 tweets so far.\n",
      "Collected 384 tweets so far.\n",
      "Collected 390 tweets so far.\n",
      "Collected 396 tweets so far.\n",
      "Collected 402 tweets so far.\n",
      "Collected 408 tweets so far.\n",
      "Collected 414 tweets so far.\n",
      "Collected 420 tweets so far.\n",
      "Collected 426 tweets so far.\n",
      "Collected 432 tweets so far.\n",
      "Collected 438 tweets so far.\n",
      "Collected 444 tweets so far.\n",
      "Collected 450 tweets so far.\n",
      "Collected 456 tweets so far.\n",
      "Collected 462 tweets so far.\n",
      "Collected 468 tweets so far.\n",
      "Collected 474 tweets so far.\n",
      "Collected 480 tweets so far.\n",
      "Collected 486 tweets so far.\n",
      "Collected 492 tweets so far.\n",
      "Collected 498 tweets so far.\n",
      "Collected 504 tweets so far.\n",
      "Collected 510 tweets so far.\n",
      "Collected 516 tweets so far.\n",
      "Collected 522 tweets so far.\n",
      "Collected 528 tweets so far.\n",
      "Collected 534 tweets so far.\n",
      "Collected 540 tweets so far.\n",
      "Collected 546 tweets so far.\n",
      "Collected 552 tweets so far.\n",
      "Collected 558 tweets so far.\n",
      "Collected 564 tweets so far.\n",
      "Collected 570 tweets so far.\n",
      "Collected 576 tweets so far.\n",
      "Collected 582 tweets so far.\n",
      "Collected 588 tweets so far.\n",
      "Collected 594 tweets so far.\n",
      "Collected 600 tweets so far.\n",
      "Collected 606 tweets so far.\n",
      "Collected 612 tweets so far.\n",
      "Collected 618 tweets so far.\n",
      "Collected 624 tweets so far.\n",
      "Collected 630 tweets so far.\n",
      "Collected 636 tweets so far.\n",
      "Collected 642 tweets so far.\n",
      "Collected 648 tweets so far.\n",
      "Collected 654 tweets so far.\n",
      "Collected 660 tweets so far.\n",
      "Collected 666 tweets so far.\n",
      "Collected 672 tweets so far.\n",
      "Collected 678 tweets so far.\n",
      "Collected 684 tweets so far.\n",
      "Collected 690 tweets so far.\n",
      "Collected 696 tweets so far.\n",
      "Collected 702 tweets so far.\n",
      "Collected 708 tweets so far.\n",
      "Collected 714 tweets so far.\n",
      "Collected 720 tweets so far.\n",
      "Collected 726 tweets so far.\n",
      "Collected 732 tweets so far.\n",
      "Collected 738 tweets so far.\n",
      "Collected 744 tweets so far.\n",
      "Collected 750 tweets so far.\n",
      "Collected 756 tweets so far.\n",
      "Collected 762 tweets so far.\n",
      "Collected 768 tweets so far.\n",
      "Collected 774 tweets so far.\n",
      "Collected 780 tweets so far.\n",
      "Collected 786 tweets so far.\n",
      "Collected 792 tweets so far.\n",
      "Collected 798 tweets so far.\n",
      "Collected 804 tweets so far.\n",
      "Collected 810 tweets so far.\n",
      "Collected 816 tweets so far.\n",
      "Collected 822 tweets so far.\n",
      "Collected 828 tweets so far.\n",
      "Collected 834 tweets so far.\n",
      "Collected 840 tweets so far.\n",
      "Collected 846 tweets so far.\n",
      "Collected 852 tweets so far.\n",
      "Collected 858 tweets so far.\n",
      "Collected 864 tweets so far.\n",
      "Collected 870 tweets so far.\n",
      "Collected 876 tweets so far.\n",
      "Collected 882 tweets so far.\n",
      "Collected 888 tweets so far.\n",
      "Collected 894 tweets so far.\n",
      "Collected 900 tweets so far.\n",
      "Collected 906 tweets so far.\n",
      "Collected 912 tweets so far.\n",
      "Collected 918 tweets so far.\n",
      "Collected 924 tweets so far.\n",
      "Collected 930 tweets so far.\n",
      "Collected 936 tweets so far.\n",
      "Collected 942 tweets so far.\n",
      "Collected 948 tweets so far.\n",
      "Collected 954 tweets so far.\n",
      "Collected 960 tweets so far.\n",
      "Collected 966 tweets so far.\n",
      "Collected 972 tweets so far.\n",
      "Collected 978 tweets so far.\n",
      "Collected 984 tweets so far.\n",
      "Collected 990 tweets so far.\n",
      "Collected 996 tweets so far.\n",
      "Collected 1000 tweets so far.\n",
      "Collected 1000 tweets.\n",
      "{'content': 'Expanded pipelines, port capacity connect #Permian oil wells to the world | #crudeoil #OOTT\\n\\n#Infographic from \\n@meghangordon\\n + \\n@sajbelcart\\n: http://plts.co/lwhR50yeMo7', 'timestamp': '2020-02-05T23:18:40.000Z'}\n",
      "{'content': \"#PlattsInfographic: How much Russian #oil does Europe import?\\n#Russia's war on #Ukraine has already disrupted more than 2 million b/d of #crude, #gasoil and #diesel, as official sanctions and industry avoidance crimp flows. #OOTT\\n\\n Get the full picture! https://okt.to/vSqb94\", 'timestamp': '2022-03-21T12:38:03.000Z'}\n",
      "{'content': 'US #crudeoil, #natgas rig count drops by 47 amid extreme activity cutbacks | #coronavirus\\n\\n* Largest single-week drop in four years\\n* #Permian takes biggest hit of 20 rigs\\n* E&Ps may make further activity cuts\\n\\nFull story: http://plts.co/KEpm50yWUIT', 'timestamp': '2020-03-26T20:20:23.000Z'}\n",
      "{'content': '#Commodities markets face significant uncertainty going into 2023. The Russia-Ukraine war & Chinese demand are likely to play a crucial role, as #recessionfears continue to weigh on US & W. Europe. Here are the key events to watch this quarter. #OOTT #OCTT #ONGT #OATT #CERAWeek', 'timestamp': '2023-01-10T11:27:04.000Z'}\n",
      "{'content': 'Saudi Arabia is positioning itself to supply more #oil to Europe as traditional buyers of Russian barrels look to diversify ahead of looming sanctions in early December and analysis suggests the trend could become more pronounced in 2023 | https://okt.to/b1ZfzX\\n\\n#OOTT #crudeoil', 'timestamp': '2022-11-14T08:30:04.000Z'}\n",
      "{'content': 'A new breed of #plastic #recycling plants capable of recovering crude and fuels from plastic waste is piling more pressure on global #oil demand forecasts. Robert Perkins reports: http://plts.co/w5xs50oPYRs | #OOTT', 'timestamp': '2019-04-02T09:07:39.000Z'}\n",
      "{'content': 'Expanded pipelines, port capacity connect #Permian oil wells to the world | #crudeoil #OOTT\\n\\n#Infographic from \\n@meghangordon\\n + \\n@sajbelcart\\n: http://plts.co/lwhR50yeMo7', 'timestamp': '2020-02-05T23:18:40.000Z'}\n",
      "{'content': \"#PlattsInfographic: How much Russian #oil does Europe import?\\n#Russia's war on #Ukraine has already disrupted more than 2 million b/d of #crude, #gasoil and #diesel, as official sanctions and industry avoidance crimp flows. #OOTT\\n\\n Get the full picture! https://okt.to/vSqb94\", 'timestamp': '2022-03-21T12:38:03.000Z'}\n",
      "{'content': 'US #crudeoil, #natgas rig count drops by 47 amid extreme activity cutbacks | #coronavirus\\n\\n* Largest single-week drop in four years\\n* #Permian takes biggest hit of 20 rigs\\n* E&Ps may make further activity cuts\\n\\nFull story: http://plts.co/KEpm50yWUIT', 'timestamp': '2020-03-26T20:20:23.000Z'}\n",
      "{'content': '#Commodities markets face significant uncertainty going into 2023. The Russia-Ukraine war & Chinese demand are likely to play a crucial role, as #recessionfears continue to weigh on US & W. Europe. Here are the key events to watch this quarter. #OOTT #OCTT #ONGT #OATT #CERAWeek', 'timestamp': '2023-01-10T11:27:04.000Z'}\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Initialize the Chrome driver\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "# Open the Twitter (X) profile page\n",
    "driver.get(\"https://x.com/spgcioil?lang=en\")\n",
    "time.sleep(5)  # Allow the page to load\n",
    "\n",
    "# Function to scroll the page and load more tweets\n",
    "def scroll_down(driver, scroll_pause_time=2):\n",
    "    last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "    \n",
    "    while True:\n",
    "        # Scroll down to the bottom of the page\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        \n",
    "        # Wait for new tweets to load\n",
    "        time.sleep(scroll_pause_time)\n",
    "        \n",
    "        # Calculate new scroll height and compare it with the last scroll height\n",
    "        new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "        if new_height == last_height:\n",
    "            break  # If the page height hasn't changed, break the loop\n",
    "        last_height = new_height\n",
    "\n",
    "# Function to scrape tweets until we get 1000\n",
    "def scrape_1000_tweets(driver, tweet_limit=1000):\n",
    "    result = []\n",
    "    wait = WebDriverWait(driver, 10)\n",
    "\n",
    "    while len(result) < tweet_limit:\n",
    "        # Scroll down the page to load more tweets\n",
    "        scroll_down(driver)\n",
    "\n",
    "        # Wait until tweet elements are loaded\n",
    "        try:\n",
    "            tweets = wait.until(EC.presence_of_all_elements_located((By.CSS_SELECTOR, 'article')))\n",
    "        except Exception as e:\n",
    "            print(\"Error locating tweets:\", e)\n",
    "            break\n",
    "\n",
    "        # Extract information from each tweet\n",
    "        for tweet in tweets:\n",
    "            if len(result) >= tweet_limit:\n",
    "                break  # Stop if we've already collected enough tweets\n",
    "            try:\n",
    "                # Extract tweet content\n",
    "                tweet_content = tweet.find_element(By.CSS_SELECTOR, 'div[lang]').text\n",
    "                \n",
    "                # Extract the timestamp\n",
    "                timestamp_element = tweet.find_element(By.TAG_NAME, 'time')\n",
    "                timestamp = timestamp_element.get_attribute('datetime')\n",
    "\n",
    "                # Append to the result if the tweet content is not already in the list\n",
    "                result.append({\n",
    "                    \"content\": tweet_content,\n",
    "                    \"timestamp\": timestamp\n",
    "                })\n",
    "            except Exception as e:\n",
    "                print(\"Error extracting tweet data:\", e)\n",
    "        \n",
    "        print(f\"Collected {len(result)} tweets so far.\")\n",
    "\n",
    "    return result\n",
    "\n",
    "# Scrape 1000 tweets\n",
    "tweets_data = scrape_1000_tweets(driver, tweet_limit=1000)\n",
    "\n",
    "# Close the driver\n",
    "driver.quit()\n",
    "\n",
    "# Print the number of tweets collected\n",
    "print(f\"Collected {len(tweets_data)} tweets.\")\n",
    "\n",
    "# Optionally, print or save the collected tweets\n",
    "for tweet in tweets_data[:10]:  # Print first 10 tweets as a preview\n",
    "    print(tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "import demoji\n",
    "\n",
    "# Example of a basic stopwords list (you can expand this list based on your needs)\n",
    "basic_stopwords = set([\n",
    "    \"i\", \"me\", \"my\", \"myself\", \"we\", \"our\", \"ours\", \"ourselves\", \"you\", \"your\", \n",
    "    \"yours\", \"he\", \"him\", \"his\", \"her\", \"she\", \"it\", \"its\", \"they\", \"them\", \n",
    "    \"their\", \"theirs\", \"what\", \"which\", \"who\", \"whom\", \"this\", \"that\", \"am\", \n",
    "    \"is\", \"are\", \"was\", \"were\", \"be\", \"been\", \"being\", \"have\", \"has\", \"had\", \n",
    "    \"having\", \"do\", \"does\", \"did\", \"but\", \"if\", \"or\", \"because\", \"as\", \"until\", \n",
    "    \"while\", \"of\", \"at\", \"by\", \"for\", \"with\", \"about\", \"against\", \"between\", \n",
    "    \"into\", \"through\", \"during\", \"before\", \"after\", \"above\", \"below\", \"to\", \n",
    "    \"from\", \"up\", \"down\", \"in\", \"out\", \"on\", \"off\", \"over\", \"under\", \"again\", \n",
    "    \"further\", \"then\", \"once\", \"here\", \"there\", \"when\", \"where\", \"why\", \"how\",\n",
    "    \"all\", \"any\", \"both\", \"each\", \"few\", \"more\", \"other\", \"some\", \"such\", \"no\",\n",
    "    \"nor\", \"not\", \"only\", \"own\", \"same\", \"so\", \"than\", \"too\", \"very\", \"s\", \"t\",\n",
    "    \"can\", \"will\", \"just\", \"don\", \"should\", \"now\"\n",
    "])\n",
    "\n",
    "# Function to clean tweet text without NLTK and remove hashtags\n",
    "def clean_text(text):\n",
    "    # Lowercase the text\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove URLs\n",
    "    text = re.sub(r\"https?://\\S+\", \"\", text)\n",
    "    \n",
    "    # Remove hashtags (words starting with #)\n",
    "    text = re.sub(r\"#\\w+\", \"\", text)\n",
    "    \n",
    "    # Remove emoji descriptions\n",
    "    text = demoji.replace_with_desc(text)\n",
    "    \n",
    "    # Remove punctuation\n",
    "    text = text.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "    \n",
    "    # Remove numbers\n",
    "    text = re.sub(r\"\\d+\", \"\", text)\n",
    "    \n",
    "    # Tokenize and remove stopwords\n",
    "    tokens = text.split()\n",
    "    tokens = [word for word in tokens if word not in basic_stopwords and len(word) > 2]\n",
    "    \n",
    "    # Return cleaned text\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "# Example tweets data\n",
    "tweets_data_test = [{'content': 'Expanded pipelines, port capacity connect #Permian oil wells to the world | #crudeoil #OOTT\\n\\n#Infographic from \\n@meghangordon\\n + \\n@sajbelcart\\n: http://plts.co/lwhR50yeMo7', 'timestamp': '2020-02-05T23:18:40.000Z'},\n",
    "    {'content': \"#PlattsInfographic: How much Russian #oil does Europe import?\\n#Russia's war on #Ukraine has already disrupted more than 2 million b/d of #crude, #gasoil and #diesel, as official sanctions and industry avoidance crimp flows. #OOTT\\n\\n Get the full picture! https://okt.to/vSqb94\", 'timestamp': '2022-03-21T12:38:03.000Z'},\n",
    "    {'content': 'US #crudeoil, #natgas rig count drops by 47 amid extreme activity cutbacks | #coronavirus\\n\\n* Largest single-week drop in four years\\n* #Permian takes biggest hit of 20 rigs\\n* E&Ps may make further activity cuts\\n\\nFull story: http://plts.co/KEpm50yWUIT', 'timestamp': '2020-03-26T20:20:23.000Z'},\n",
    "    {'content': '#Commodities markets face significant uncertainty going into 2023. The Russia-Ukraine war & Chinese demand are likely to play a crucial role, as #recessionfears continue to weigh on US & W. Europe. Here are the key events to watch this quarter. #OOTT #OCTT #ONGT #OATT #CERAWeek', 'timestamp': '2023-01-10T11:27:04.000Z'},\n",
    "    {'content': 'Saudi Arabia is positioning itself to supply more #oil to Europe as traditional buyers of Russian barrels look to diversify ahead of looming sanctions in early December and analysis suggests the trend could become more pronounced in 2023 | https://okt.to/b1ZfzX\\n\\n#OOTT #crudeoil', 'timestamp': '2022-11-14T08:30:04.000Z'},\n",
    "    {'content': 'A new breed of #plastic #recycling plants capable of recovering crude and fuels from plastic waste is piling more pressure on global #oil demand forecasts. Robert Perkins reports: http://plts.co/w5xs50oPYRs | #OOTT', 'timestamp': '2019-04-02T09:07:39.000Z'},\n",
    "    {'content': 'Expanded pipelines, port capacity connect #Permian oil wells to the world | #crudeoil #OOTT\\n\\n#Infographic from \\n@meghangordon\\n + \\n@sajbelcart\\n: http://plts.co/lwhR50yeMo7', 'timestamp': '2020-02-05T23:18:40.000Z'},\n",
    "    {'content': \"#PlattsInfographic: How much Russian #oil does Europe import?\\n#Russia's war on #Ukraine has already disrupted more than 2 million b/d of #crude, #gasoil and #diesel, as official sanctions and industry avoidance crimp flows. #OOTT\\n\\n Get the full picture! https://okt.to/vSqb94\", 'timestamp': '2022-03-21T12:38:03.000Z'},\n",
    "    {'content': 'US #crudeoil, #natgas rig count drops by 47 amid extreme activity cutbacks | #coronavirus\\n\\n* Largest single-week drop in four years\\n* #Permian takes biggest hit of 20 rigs\\n* E&Ps may make further activity cuts\\n\\nFull story: http://plts.co/KEpm50yWUIT', 'timestamp': '2020-03-26T20:20:23.000Z'},\n",
    "    {'content': '#Commodities markets face significant uncertainty going into 2023. The Russia-Ukraine war & Chinese demand are likely to play a crucial role, as #recessionfears continue to weigh on US & W. Europe. Here are the key events to watch this quarter. #OOTT #OCTT #ONGT #OATT #CERAWeek', 'timestamp': '2023-01-10T11:27:04.000Z'},\n",
    "]\n",
    "# Load the oil_energy_data.csv file\n",
    "oil_data = pd.read_csv('oil_energy_data.csv')\n",
    "\n",
    "# Create a list to store cleaned tweets\n",
    "cleaned_tweets = []\n",
    "\n",
    "# Clean the tweet content and format the data\n",
    "for tweet in tweets_data:\n",
    "    cleaned_tweet = clean_text(tweet['content'])\n",
    "    cleaned_tweets.append({\n",
    "        'url': '',  # Blank for tweets\n",
    "        'title': cleaned_tweet,\n",
    "        'date': tweet['timestamp']\n",
    "    })\n",
    "\n",
    "# Convert cleaned tweets to DataFrame\n",
    "tweets_df = pd.DataFrame(cleaned_tweets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>title</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td></td>\n",
       "      <td>expanded pipelines port capacity connect oil w...</td>\n",
       "      <td>2020-02-05T23:18:40.000Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td></td>\n",
       "      <td>much russian europe import war already disrupt...</td>\n",
       "      <td>2022-03-21T12:38:03.000Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td></td>\n",
       "      <td>rig count drops amid extreme activity cutbacks...</td>\n",
       "      <td>2020-03-26T20:20:23.000Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td></td>\n",
       "      <td>markets face significant uncertainty going the...</td>\n",
       "      <td>2023-01-10T11:27:04.000Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td></td>\n",
       "      <td>saudi arabia positioning itself supply europe ...</td>\n",
       "      <td>2022-11-14T08:30:04.000Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td></td>\n",
       "      <td>new breed plants capable recovering crude and ...</td>\n",
       "      <td>2019-04-02T09:07:39.000Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td></td>\n",
       "      <td>expanded pipelines port capacity connect oil w...</td>\n",
       "      <td>2020-02-05T23:18:40.000Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td></td>\n",
       "      <td>much russian europe import war already disrupt...</td>\n",
       "      <td>2022-03-21T12:38:03.000Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td></td>\n",
       "      <td>rig count drops amid extreme activity cutbacks...</td>\n",
       "      <td>2020-03-26T20:20:23.000Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td></td>\n",
       "      <td>markets face significant uncertainty going the...</td>\n",
       "      <td>2023-01-10T11:27:04.000Z</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    url                                              title  \\\n",
       "0        expanded pipelines port capacity connect oil w...   \n",
       "1        much russian europe import war already disrupt...   \n",
       "2        rig count drops amid extreme activity cutbacks...   \n",
       "3        markets face significant uncertainty going the...   \n",
       "4        saudi arabia positioning itself supply europe ...   \n",
       "..   ..                                                ...   \n",
       "995      new breed plants capable recovering crude and ...   \n",
       "996      expanded pipelines port capacity connect oil w...   \n",
       "997      much russian europe import war already disrupt...   \n",
       "998      rig count drops amid extreme activity cutbacks...   \n",
       "999      markets face significant uncertainty going the...   \n",
       "\n",
       "                         date  \n",
       "0    2020-02-05T23:18:40.000Z  \n",
       "1    2022-03-21T12:38:03.000Z  \n",
       "2    2020-03-26T20:20:23.000Z  \n",
       "3    2023-01-10T11:27:04.000Z  \n",
       "4    2022-11-14T08:30:04.000Z  \n",
       "..                        ...  \n",
       "995  2019-04-02T09:07:39.000Z  \n",
       "996  2020-02-05T23:18:40.000Z  \n",
       "997  2022-03-21T12:38:03.000Z  \n",
       "998  2020-03-26T20:20:23.000Z  \n",
       "999  2023-01-10T11:27:04.000Z  \n",
       "\n",
       "[1000 rows x 3 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/changchoichang/Library/Mobile Documents/com~apple~CloudDocs/Documents/GitHub/oil-price-prediction/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Analyzing sentiment: 100%|██████████| 3339/3339 [01:07<00:00, 49.39 rows/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New CSV file 'aggregated_oil_energy_sentiment_filtered.csv' has been created. Sentiment analysis completed successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "/var/folders/0p/lnktjx254_z2nwf92thzm_hr0000gn/T/ipykernel_70647/1167764178.py:78: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df['sentiment_score'].fillna(0, inplace=True)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "\n",
    "# Function to load the tokenizer and model\n",
    "def load_model():\n",
    "    try:\n",
    "        # Load tokenizer and model\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
    "        return tokenizer, model\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading model: {e}\")\n",
    "        return None, None\n",
    "\n",
    "# Function to perform sentiment analysis on a text input\n",
    "def analyze_sentiment(text, tokenizer, model):\n",
    "    try:\n",
    "        # Tokenize the input text\n",
    "        inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "        \n",
    "        # Perform the prediction\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "        \n",
    "        # Get the logits (model's raw output)\n",
    "        logits = outputs.logits\n",
    "        \n",
    "        # Convert logits to sentiment prediction\n",
    "        sentiment = torch.argmax(logits, dim=1).item()\n",
    "        \n",
    "        # Define the sentiment categories\n",
    "        sentiment_map = {0: -1, 1: 1}  # Adjusting to return -1 for negative, +1 for positive\n",
    "        \n",
    "        # Return the corresponding sentiment score\n",
    "        return sentiment_map[sentiment]\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error during sentiment analysis: {e}\")\n",
    "        return None\n",
    "\n",
    "# Function to process the CSV and apply sentiment analysis\n",
    "def process_csv(file_path):\n",
    "    # Load the tokenizer and model\n",
    "    tokenizer, model = load_model()\n",
    "    \n",
    "    # Check if the model was loaded successfully\n",
    "    if tokenizer is None or model is None:\n",
    "        print(\"Model loading failed. Exiting.\")\n",
    "        return\n",
    "    \n",
    "    # Load the CSV file\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Convert the 'date' column to datetime, handling different formats\n",
    "    df['date'] = pd.to_datetime(df['date'], errors='coerce')  # Convert to datetime, invalid dates will be NaT\n",
    "    \n",
    "    # Filter out rows with dates earlier than 2019-04-02\n",
    "    filter_date = pd.to_datetime(\"2019-04-02\")\n",
    "    df = df[df['date'] >= filter_date]\n",
    "    \n",
    "    # Initialize the tqdm progress bar\n",
    "    tqdm_bar = tqdm(total=len(df), desc=\"Analyzing sentiment\", unit=\" rows\")\n",
    "    \n",
    "    # Apply sentiment analysis to each title with a progress bar\n",
    "    df['sentiment_score'] = df['title'].apply(lambda x: analyze_sentiment(x, tokenizer, model))\n",
    "    \n",
    "    # Update the progress bar after each row is processed\n",
    "    for _ in df.index:\n",
    "        tqdm_bar.update(1)\n",
    "    \n",
    "    # Close the progress bar after completion\n",
    "    tqdm_bar.close()\n",
    "    \n",
    "    # If any sentiment analysis failed, fill with 0 (neutral)\n",
    "    df['sentiment_score'].fillna(0, inplace=True)\n",
    "    \n",
    "    # Aggregating data by date\n",
    "    aggregated_df = df.groupby('date').agg(\n",
    "        P_average=('sentiment_score', 'mean'),\n",
    "        P_sum=('sentiment_score', 'sum'),\n",
    "        news_count=('title', 'count')  # Counting the number of news articles\n",
    "    ).reset_index()\n",
    "    \n",
    "    # Sort the result by date (from farthest to nearest)\n",
    "    aggregated_df = aggregated_df.sort_values(by='date', ascending=False)\n",
    "    \n",
    "    # Save the aggregated data into a new CSV file\n",
    "    output_file = 'aggregated_oil_energy_sentiment_filtered.csv'\n",
    "    aggregated_df.to_csv(output_file, index=False)\n",
    "    \n",
    "    # Print a message when done\n",
    "    print(f\"New CSV file '{output_file}' has been created. Sentiment analysis completed successfully.\")\n",
    "\n",
    "# Main function to run the process\n",
    "if __name__ == \"__main__\":\n",
    "    # Path to the input CSV file\n",
    "    file_path = 'updated_oil_energy_data.csv'\n",
    "    \n",
    "    # Process the CSV and perform sentiment analysis\n",
    "    process_csv(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/changchoichang/Library/Mobile Documents/com~apple~CloudDocs/Documents/GitHub/oil-price-prediction/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Analyzing sentiment: 100%|██████████| 4339/4339 [01:21<00:00, 53.15 rows/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New CSV file 'oil_energy_news_sentiment_per_article.csv' has been created. Sentiment analysis completed successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "/var/folders/0p/lnktjx254_z2nwf92thzm_hr0000gn/T/ipykernel_70647/3563456025.py:94: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df['sentiment_score'].fillna(0, inplace=True)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "\n",
    "# Function to load the tokenizer and model\n",
    "def load_model():\n",
    "    try:\n",
    "        # Load tokenizer and model\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
    "        return tokenizer, model\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading model: {e}\")\n",
    "        return None, None\n",
    "\n",
    "# Function to perform sentiment analysis on a text input\n",
    "def analyze_sentiment(text, tokenizer, model):\n",
    "    try:\n",
    "        # Tokenize the input text\n",
    "        inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "        \n",
    "        # Perform the prediction\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "        \n",
    "        # Get the logits (model's raw output)\n",
    "        logits = outputs.logits\n",
    "        \n",
    "        # Convert logits to sentiment prediction\n",
    "        sentiment = torch.argmax(logits, dim=1).item()\n",
    "        \n",
    "        # Define the sentiment categories\n",
    "        sentiment_map = {0: -1, 1: 1}  # Adjusting to return -1 for negative, +1 for positive\n",
    "        \n",
    "        # Return the corresponding sentiment score\n",
    "        return sentiment_map[sentiment]\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error during sentiment analysis: {e}\")\n",
    "        return None\n",
    "\n",
    "# Function to process the CSV and apply sentiment analysis\n",
    "def process_csv(file_path):\n",
    "    # Load the tokenizer and model\n",
    "    tokenizer, model = load_model()\n",
    "    \n",
    "    # Check if the model was loaded successfully\n",
    "    if tokenizer is None or model is None:\n",
    "        print(\"Model loading failed. Exiting.\")\n",
    "        return\n",
    "    \n",
    "    # Load the CSV file\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Function to convert the date to the desired format\n",
    "    def format_date(date_str):\n",
    "        try:\n",
    "            # First convert the string into a datetime object\n",
    "            dt = pd.to_datetime(date_str, errors='coerce')\n",
    "            if pd.isnull(dt):\n",
    "                return None  # Return None if the date is not valid\n",
    "            # Format the date to \"Aug 12, 2022 at 08:31\" style\n",
    "            return dt.strftime(\"%b %d, %Y at %H:%M\")\n",
    "        except Exception as e:\n",
    "            return None\n",
    "    \n",
    "    # Apply the date formatting function\n",
    "    df['date'] = df['date'].apply(format_date)\n",
    "    \n",
    "    # Remove rows where the date conversion failed\n",
    "    df = df.dropna(subset=['date'])\n",
    "    \n",
    "    # Filter out rows with dates earlier than 2019-04-02\n",
    "    filter_date = pd.to_datetime(\"2019-04-02\")\n",
    "    df['datetime'] = pd.to_datetime(df['date'], format=\"%b %d, %Y at %H:%M\", errors='coerce')\n",
    "    df = df[df['datetime'] >= filter_date]\n",
    "    \n",
    "    # Initialize the tqdm progress bar\n",
    "    tqdm_bar = tqdm(total=len(df), desc=\"Analyzing sentiment\", unit=\" rows\")\n",
    "    \n",
    "    # Apply sentiment analysis to each title with a progress bar\n",
    "    df['sentiment_score'] = df['title'].apply(lambda x: analyze_sentiment(x, tokenizer, model))\n",
    "    \n",
    "    # Update the progress bar after each row is processed\n",
    "    for _ in df.index:\n",
    "        tqdm_bar.update(1)\n",
    "    \n",
    "    # Close the progress bar after completion\n",
    "    tqdm_bar.close()\n",
    "    \n",
    "    # If any sentiment analysis failed, fill with 0 (neutral)\n",
    "    df['sentiment_score'].fillna(0, inplace=True)\n",
    "    \n",
    "    # Drop the 'datetime' column before saving to keep the original format\n",
    "    df = df.drop(columns=['datetime'])\n",
    "    \n",
    "    # Sort the result by date and time\n",
    "    df = df.sort_values(by='date', ascending=False)\n",
    "    \n",
    "    # Save the processed data into a new CSV file\n",
    "    output_file = 'oil_energy_news_sentiment_per_article.csv'\n",
    "    df.to_csv(output_file, index=False)\n",
    "    \n",
    "    # Print a message when done\n",
    "    print(f\"New CSV file '{output_file}' has been created. Sentiment analysis completed successfully.\")\n",
    "\n",
    "# Main function to run the process\n",
    "if __name__ == \"__main__\":\n",
    "    # Path to the input CSV file\n",
    "    file_path = 'updated_oil_energy_data.csv'\n",
    "    \n",
    "    # Process the CSV and perform sentiment analysis\n",
    "    process_csv(file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New CSV file 'aggregated_oil_energy_news_sentiment.csv' has been created with aggregated sentiment data.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Function to process the CSV and calculate the required metrics\n",
    "def post_process_csv(file_path):\n",
    "    # Load the CSV file\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Function to convert the date to the desired format\n",
    "    def format_date(date_str):\n",
    "        try:\n",
    "            # Convert the string into a datetime object\n",
    "            dt = pd.to_datetime(date_str, errors='coerce')\n",
    "            if pd.isnull(dt):\n",
    "                return None  # Return None if the date is not valid\n",
    "            # Format the date to \"YYYY-MM-DD\" style\n",
    "            return dt.strftime(\"%Y-%m-%d\")\n",
    "        except Exception as e:\n",
    "            return None\n",
    "    \n",
    "    # Apply the date formatting function\n",
    "    df['date'] = df['date'].apply(format_date)\n",
    "    \n",
    "    # Remove rows where the date conversion failed\n",
    "    df = df.dropna(subset=['date'])\n",
    "    \n",
    "    # Filter out rows with dates earlier than 2019-04-02\n",
    "    filter_date = pd.to_datetime(\"2019-04-02\")\n",
    "    df['date'] = pd.to_datetime(df['date'], format=\"%Y-%m-%d\")\n",
    "    df = df[df['date'] >= filter_date]\n",
    "    \n",
    "    # Group the data by date and calculate P_mean, P_sum, and Count_news\n",
    "    aggregated_df = df.groupby('date').agg(\n",
    "        P_mean=('sentiment_score', 'mean'),\n",
    "        P_sum=('sentiment_score', 'sum'),\n",
    "        Count_news=('sentiment_score', 'size')\n",
    "    ).reset_index()\n",
    "    \n",
    "    # Save the aggregated data into a new CSV file\n",
    "    output_file = 'aggregated_oil_energy_news_sentiment.csv'\n",
    "    aggregated_df.to_csv(output_file, index=False)\n",
    "    \n",
    "    # Print a message when done\n",
    "    print(f\"New CSV file '{output_file}' has been created with aggregated sentiment data.\")\n",
    "\n",
    "# Main function to run the process\n",
    "if __name__ == \"__main__\":\n",
    "    # Path to the input CSV file\n",
    "    file_path = 'oil_energy_news_sentiment_per_article.csv'\n",
    "    \n",
    "    # Process the CSV and calculate P_mean, P_sum, and Count_news\n",
    "    post_process_csv(file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First few rows of 'Date' column before conversion: 0    2007-07-30 00:00:00-04:00\n",
      "1    2007-07-31 00:00:00-04:00\n",
      "2    2007-08-01 00:00:00-04:00\n",
      "3    2007-08-02 00:00:00-04:00\n",
      "4    2007-08-03 00:00:00-04:00\n",
      "Name: Date, dtype: object\n",
      "First few rows of 'Date' column after conversion: 0    2007-07-30\n",
      "1    2007-07-31\n",
      "2    2007-08-01\n",
      "3    2007-08-02\n",
      "4    2007-08-03\n",
      "Name: Date, dtype: object\n",
      "Final merged data has been saved to 'final_oil_data.csv'.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Function to process the oil price data and merge it with the sentiment data\n",
    "def process_and_merge_data(oil_price_file, sentiment_file, output_file):\n",
    "    # Load the oil price data\n",
    "    oil_price_df = pd.read_csv(oil_price_file)\n",
    "\n",
    "    # Print the first few rows of the 'Date' column to help debug\n",
    "    print(\"First few rows of 'Date' column before conversion:\", oil_price_df['Date'].head())\n",
    "\n",
    "    # Ensure the 'Date' column is properly formatted by forcing conversion to datetime\n",
    "    oil_price_df['Date'] = pd.to_datetime(oil_price_df['Date'], errors='coerce', utc=True)\n",
    "\n",
    "    # Drop any rows with invalid dates (where conversion failed)\n",
    "    oil_price_df = oil_price_df.dropna(subset=['Date'])\n",
    "\n",
    "    # Keep only the date (remove time part) after conversion\n",
    "    oil_price_df['Date'] = oil_price_df['Date'].dt.date\n",
    "\n",
    "    # Print the first few rows of 'Date' column after conversion for verification\n",
    "    print(\"First few rows of 'Date' column after conversion:\", oil_price_df['Date'].head())\n",
    "\n",
    "    # Load the sentiment data\n",
    "    sentiment_df = pd.read_csv(sentiment_file)\n",
    "\n",
    "    # Ensure the 'date' column in the sentiment data is properly formatted\n",
    "    sentiment_df['date'] = pd.to_datetime(sentiment_df['date'], errors='coerce').dt.date\n",
    "\n",
    "    # Drop rows with invalid date values in the sentiment data\n",
    "    sentiment_df = sentiment_df.dropna(subset=['date'])\n",
    "\n",
    "    # Merge the two dataframes on the date\n",
    "    merged_df = pd.merge(oil_price_df, sentiment_df, left_on='Date', right_on='date', how='inner')\n",
    "\n",
    "    # Drop the redundant 'date' column after merging\n",
    "    merged_df = merged_df.drop(columns=['date'])\n",
    "\n",
    "    # Save the final merged dataframe to a new CSV file\n",
    "    merged_df.to_csv(output_file, index=False)\n",
    "\n",
    "    print(f\"Final merged data has been saved to '{output_file}'.\")\n",
    "\n",
    "# Main function to run the process\n",
    "if __name__ == \"__main__\":\n",
    "    # File paths\n",
    "    oil_price_file = 'oil_price.csv'\n",
    "    sentiment_file = 'aggregated_oil_energy_news_sentiment.csv'\n",
    "    output_file = 'final_oil_data.csv'\n",
    "\n",
    "    # Run the process\n",
    "    process_and_merge_data(oil_price_file, sentiment_file, output_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
